{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T12:59:57.508668Z",
     "start_time": "2019-01-20T12:59:56.780174Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import csv, json\n",
    "import ogr, osr\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib import pyplot as plt, figure\n",
    "import copy\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "import overpy\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ogr.UseExceptions()\n",
    "osr.UseExceptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T12:59:58.641669Z",
     "start_time": "2019-01-20T12:59:58.609379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parent directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Child directories\n",
    "boundaries_dir = os.path.join(cwd, 'boundaries')\n",
    "gtfs_dir = os.path.join(cwd, 'gtfs')\n",
    "output_dir = os.path.join(cwd, 'output')\n",
    "\n",
    "# Create output directory\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:00.205886Z",
     "start_time": "2019-01-20T13:00:00.082118Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dict_to_geojson(data, out_path, geom_field, fields_key=None, epsg_id=None):\n",
    "    \n",
    "    # Create path\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "        \n",
    "    # Get GeoJSON driver\n",
    "    driver = ogr.GetDriverByName('GeoJSON')\n",
    "    \n",
    "    ds = driver.CreateDataSource(out_path)\n",
    "    \n",
    "    spatial_ref = osr.SpatialReference()\n",
    "    if epsg_id:\n",
    "        spatial_ref.ImportFromEPSG(4326)\n",
    "    else:\n",
    "        spatial_ref.ImportFromEPSG(epsg_id)\n",
    "        \n",
    "    # Get geom type\n",
    "    geom_type = data[0][geom_field].GetGeometryType()\n",
    "    \n",
    "    layer = ds.CreateLayer(out_path, geom_type=geom_type, srs=spatial_ref)\n",
    "    \n",
    "    # Create the fields\n",
    "    if fields_key:\n",
    "        field_names = data[0][fields_key].keys()\n",
    "        for i, field_name in enumerate(field_names): \n",
    "            layer.CreateField(ogr.FieldDefn(field_name, ogr.OFTString))\n",
    "    \n",
    "    layer_defn = layer.GetLayerDefn()\n",
    "    \n",
    "    for item in data:\n",
    "        feature = ogr.Feature(layer_defn)\n",
    "        \n",
    "        if fields_key:\n",
    "            for field_name in field_names:\n",
    "                feature.SetField(field_name, item[fields_key][field_name])\n",
    "        \n",
    "        feature.SetGeometry(item[geom_field])\n",
    "        \n",
    "        layer.CreateFeature(feature)\n",
    "        \n",
    "def objects_to_xml(path, bounds=None, nodes=None, ways=None, relations=None):\n",
    "    root = ET.Element(\"osm\")\n",
    "    pass\n",
    "\n",
    "def reproject_geometry(geom, in_epsg, out_epsg, return_wkt=False):\n",
    "    import ogr, osr\n",
    "\n",
    "    source = osr.SpatialReference()\n",
    "    source.ImportFromEPSG(in_epsg)\n",
    "\n",
    "    target = osr.SpatialReference()\n",
    "    target.ImportFromEPSG(out_epsg)\n",
    "\n",
    "    transform = osr.CoordinateTransformation(source, target)\n",
    "\n",
    "    geom.Transform(transform)\n",
    "\n",
    "    if return_wkt:\n",
    "        return geom.ExportToWkt()\n",
    "    else:\n",
    "        return geom\n",
    "    \n",
    "def write_geometry_to_geojson(geom, out_path):\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    \n",
    "    driver = ogr.GetDriverByName('GeoJSON')\n",
    "    ds = driver.CreateDataSource(out_path)\n",
    "    \n",
    "    geom_type = geom.GetGeometryType()\n",
    "    \n",
    "    layer = ds.CreateLayer(out_path, geom_type=geom_type)\n",
    "    layer_defn = layer.GetLayerDefn()\n",
    "    \n",
    "    feature = ogr.Feature(layer_defn)\n",
    "    feature.SetGeometry(geom)\n",
    "    layer.CreateFeature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:18:14.967932Z",
     "start_time": "2019-01-13T08:18:14.957921Z"
    }
   },
   "source": [
    "### Overpy Query Templates\n",
    "\n",
    "*Overpass doesn't allow Geocoding like Turbo Overpass does so I've gathered the city relation ids beforehand to use directly with overpass*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:01.662953Z",
     "start_time": "2019-01-20T13:00:01.648877Z"
    }
   },
   "outputs": [],
   "source": [
    "api = overpy.Overpass()\n",
    "\n",
    "relation_to_area_factor = 3600000000\n",
    "\n",
    "region_ids = {\n",
    "    \"Laval\": 3532125 + relation_to_area_factor,\n",
    "    \"Montreal\": 1571328 + relation_to_area_factor\n",
    "}\n",
    "\n",
    "\n",
    "tmpl = \"\"\"\n",
    "    area({})->.searchArea;\n",
    "    (\n",
    "      node[\"amenity\"=\"pub\"](area.searchArea);\n",
    "      way[\"amenity\"=\"pub\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"pub\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bus_stop_tmpl = \"\"\"\n",
    "    area({})->.searchArea;\n",
    "    (\n",
    "    node[\"highway\"=\"bus_stop\"](area.searchArea);\n",
    "    way[\"highway\"=\"platform\"](area.searchArea);\n",
    "\n",
    "    node[\"public_transport\"=\"platform\"][\"bus\"=\"yes\"](area.searchArea);\n",
    "    node[\"public_transport\"=\"stop_position\"][\"bus\"=\"yes\"](area.searchArea);\n",
    "    \n",
    "    way[\"amenity\"=\"shelter\"](area.searchArea);\n",
    "    node[\"amenity\"=\"shelter\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "\"\"\"\n",
    "\n",
    "service_route_tmpl = \"\"\"\n",
    "    area({})->.searchArea;\n",
    "    (\n",
    "    relation[\"type\"=\"route\"][\"route\"=\"bus\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Boundaries into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:03.005181Z",
     "start_time": "2019-01-20T13:00:02.908008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f657d22af35f448e9fb86510a8515cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "boundaries = {}\n",
    "\n",
    "boundary_files = os.listdir(boundaries_dir)\n",
    "\n",
    "for boundary_file in tqdm_notebook(boundary_files):\n",
    "    path = os.path.join(boundaries_dir, boundary_file)\n",
    "    city = boundary_file[:-4]\n",
    "    \n",
    "    with open(path) as f:\n",
    "        geom = ogr.CreateGeometryFromWkt(f.read())\n",
    "        boundaries[city] = geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get existing data from OSM using OverPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:11.234979Z",
     "start_time": "2019-01-20T13:00:04.829036Z"
    }
   },
   "outputs": [],
   "source": [
    "existing_stops = []\n",
    "\n",
    "stops_result_laval = api.query(bus_stop_tmpl.format(region_ids['Laval']))\n",
    "stops_result_montreal = api.query(bus_stop_tmpl.format(region_ids['Montreal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:13.562648Z",
     "start_time": "2019-01-20T13:00:13.541346Z"
    }
   },
   "outputs": [],
   "source": [
    "for node in stops_result_laval.nodes:\n",
    "    existing_stop = {\n",
    "        'id': node.id,\n",
    "        'lat': node.lat,\n",
    "        'lon': node.lon,\n",
    "        'tags': node.tags,\n",
    "        'city': 'Laval'\n",
    "    }\n",
    "    existing_stops.append(existing_stop)\n",
    "    \n",
    "for node in stops_result_montreal.nodes:\n",
    "    existing_stop = {\n",
    "        'id': node.id,\n",
    "        'lat': node.lat,\n",
    "        'lon': node.lon,\n",
    "        'tags': node.tags,\n",
    "        'city': 'Montreal'\n",
    "    }\n",
    "    existing_stops.append(existing_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GTFS text files to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:19.235932Z",
     "start_time": "2019-01-20T13:00:15.604208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7eae70290b4420bad14ae50635645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gtfs_data = {}\n",
    "\n",
    "filenames = os.listdir(gtfs_dir)\n",
    "\n",
    "for filename in tqdm_notebook(filenames):\n",
    "    table_name = filename[:-4]\n",
    "    path = os.path.join(gtfs_dir, filename)\n",
    "    gtfs_data[table_name] = {\n",
    "        \"path\": path,\n",
    "    }\n",
    "\n",
    "    with open(path, encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        field_names = next(reader)\n",
    "        gtfs_data[table_name][\"field_names\"] = field_names\n",
    "        \n",
    "        first_data_row = next(reader)\n",
    "        field_types = []\n",
    "        for i, item in enumerate(first_data_row):\n",
    "            if len(item) == 1:\n",
    "                field_types.append(ogr.OFTInteger)\n",
    "            elif 'lat' in field_names[i] or 'lon' in field_names[i]:\n",
    "                field_types.append(ogr.OFTReal)\n",
    "            else:\n",
    "                field_types.append(ogr.OFTString)\n",
    "        gtfs_data[table_name][\"field_types\"] = field_types\n",
    "        \n",
    "        dict_reader = csv.DictReader(csvfile, fieldnames=field_names)\n",
    "        data = [row for row in dict_reader]\n",
    "        gtfs_data[table_name][\"data\"] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the dictionary information for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:22.913648Z",
     "start_time": "2019-01-20T13:00:22.901351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# table: # field names\n",
      "stop_times: ['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'pickup_type', 'drop_off_type']\n",
      "[4, 4, 4, 4, 0, 0, 0]\n",
      "\n",
      "calendar: ['service_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'start_date', 'end_date']\n",
      "[4, 0, 0, 0, 0, 0, 0, 0, 4, 4]\n",
      "\n",
      "trips: ['route_id', 'service_id', 'trip_id', 'block_id', 'shape_id', 'trip_headsign']\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "\n",
      "routes: ['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type', 'route_url', 'route_headsign', 'route_color', 'route_text_color']\n",
      "[4, 4, 4, 2, 0, 4, 4, 4, 4]\n",
      "\n",
      "stops: ['stop_id', 'stop_code', 'stop_name', 'stop_lon', 'stop_lat', 'location_type', 'stop_display', 'stop_abribus']\n",
      "[4, 4, 4, 2, 2, 0, 0, 0]\n",
      "\n",
      "agency: ['agency_id', 'agency_name', 'agency_url', 'agency_timezone', 'agency_lang']\n",
      "[4, 4, 4, 4, 4]\n",
      "\n",
      "calendar_dates: ['service_id', 'date', 'exception_type']\n",
      "[4, 4, 0]\n",
      "\n",
      "shapes: ['shape_id', 'shape_pt_lat', 'shape_pt_lon', 'shape_pt_sequence']\n",
      "[4, 2, 2, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('# table: # field names')\n",
    "for key, value in gtfs_data.items():\n",
    "    print('{}: {}\\n{}\\n'.format(key, value['field_names'], value['field_types']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate the stops file\n",
    "1. Split the stop code from the name\n",
    "2. Check for name uniqueness\n",
    "3. Check for location uniqueness (with proximity tolerance - optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the stop **names** and **codes** frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:24.983454Z",
     "start_time": "2019-01-20T13:00:24.887153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9001e8743faa437c819544c57e90e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stops = gtfs_data['stops']['data']\n",
    "\n",
    "unique_stops = []\n",
    "stop_name_counter = Counter()\n",
    "stop_code_counter = Counter()\n",
    "\n",
    "for stop in tqdm_notebook(stops):\n",
    "    real_name = stop['stop_name'].split('[')[0].strip()\n",
    "    stop_code = stop['stop_code']\n",
    "    stop_name_counter[real_name] += 1\n",
    "    stop_code_counter[stop_code] += 1\n",
    "    \n",
    "names, name_counts = zip(*stop_name_counter.items())\n",
    "codes, code_counts = zip(*stop_code_counter.items())\n",
    "\n",
    "bar_height = 10\n",
    "\n",
    "plot_specs = {\n",
    "    'names': {\n",
    "        'positions': np.arange(len(names)),\n",
    "        'bar_spacing': 2 * np.arange(len(names)) * bar_height,\n",
    "        'labels': names,\n",
    "        'values': name_counts,\n",
    "        'title': 'Distribution of stop names'\n",
    "    },\n",
    "    'codes': {\n",
    "        'positions': np.arange(len(codes)),\n",
    "        'bar_spacing': 2 * np.arange(len(codes)) * bar_height,\n",
    "        'labels': codes,\n",
    "        'values': code_counts,\n",
    "        'title': 'Distribution of stop codes'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:00:25.542338Z",
     "start_time": "2019-01-20T13:00:25.529513Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Can't get this to work....moving on\n",
    "\n",
    "# # plt.figure(figsize=(50,30))\n",
    "# plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# for i, (plot_name, spec) in enumerate(plot_specs.items(), 1):\n",
    "#     plt.subplot(1, 2, i)\n",
    "#     plt.barh(spec['positions'], spec['values'], height=bar_height)\n",
    "#     plt.yticks(spec['bar_spacing'], spec['labels'])\n",
    "#     plt.title(spec['title'])\n",
    "#     plt.autoscale()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T13:30:29.191793Z",
     "start_time": "2019-01-12T13:30:29.180259Z"
    }
   },
   "source": [
    "#### Create unique stops list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:04:58.426252Z",
     "start_time": "2019-01-20T13:00:26.874561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a02151204fe4c6dbba2d5e9894bb116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2773), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stops = gtfs_data['stops']['data']\n",
    "\n",
    "# existing_count_laval = 0\n",
    "# existing_count_montreal = 0\n",
    "existing_count_total = 0\n",
    "\n",
    "# Extent\n",
    "multi_point = ogr.Geometry(ogr.wkbMultiPoint)\n",
    "\n",
    "# Dictionary to group stops by unique location\n",
    "unique_locations = defaultdict(list)\n",
    "\n",
    "# List of stops rearranged with uniqueness\n",
    "unique_stops = []\n",
    "# unique_stops_gtfs = []\n",
    "# unique_stops_osm = []\n",
    "# unique_stops_map = []  # GTFS -> OSM\n",
    "\n",
    "# Aggregate unique locations\n",
    "for stop in stops:\n",
    "    unique_locations[(stop['stop_lon'], stop['stop_lat'])].append(stop)\n",
    "\n",
    "# Aggregate attributes using unique locations into new stop objects\n",
    "for i, (unique_location, stops) in enumerate(tqdm_notebook(unique_locations.items()), 1):\n",
    "\n",
    "    ids = list(set([stop['stop_id'] for stop in stops]))\n",
    "    codes = list(set([stop['stop_code'] for stop in stops]))\n",
    "\n",
    "    lon = float(stops[0]['stop_lon'])\n",
    "    lat = float(stops[0]['stop_lat'])\n",
    "\n",
    "    # Create point to add to MultiPoint (for JOSM extent later...)\n",
    "    point = ogr.Geometry(ogr.wkbPoint)\n",
    "    point.AddPoint(lon, lat)\n",
    "    multi_point.AddGeometry(point)\n",
    "\n",
    "#     # Create the unique stop maintaining GTFS file format\n",
    "#     unique_stop_gtfs = {\n",
    "#\n",
    "#         'geom': point\n",
    "#     }\n",
    "\n",
    "    # Create the unique stop according to JOSM format\n",
    "\n",
    "    action = None\n",
    "\n",
    "    # While checking for:\n",
    "    # 1. What city it is ine\n",
    "    # 2. If a node with the same geometry already exists in OSM\n",
    "    # 2.1 If node is in Laval, replace the tags\n",
    "    # 2.2 If node is in Montreal, append, code and name tags\n",
    "\n",
    "    # Get the city of the GTFS stop\n",
    "    city = None\n",
    "    for city_name, city_geom in boundaries.items():\n",
    "        if point.Intersects(city_geom):\n",
    "            city = city_name\n",
    "\n",
    "    osm_id = None\n",
    "    osm_lat = None\n",
    "    osm_lon = None\n",
    "\n",
    "    # Determine if node already exists\n",
    "    for existing_stop in existing_stops:\n",
    "        existing_point = ogr.Geometry(ogr.wkbPoint)\n",
    "        existing_point.AddPoint(\n",
    "            float(existing_stop['lon']), float(existing_stop['lat']))\n",
    "\n",
    "        if point.Equals(existing_point):\n",
    "            #             print('Equal point found')\n",
    "            existing_count_total += 1\n",
    "\n",
    "            osm_id = existing_stop['id']\n",
    "            osm_lat = existing_stop['lat']\n",
    "            osm_lon = existing_stop['lon']\n",
    "            action = 'modify'\n",
    "\n",
    "#             print(osm_id)\n",
    "        else:\n",
    "            osm_id = str(i * -1)\n",
    "            osm_lat = str(lat)\n",
    "            osm_lon = str(lon)\n",
    "\n",
    "    unique_stop = {\n",
    "        # props\n",
    "        'props': {\n",
    "            'id': osm_id,\n",
    "            'lon': osm_lon,\n",
    "            'lat': osm_lat,\n",
    "        },\n",
    "        # tags\n",
    "        'tags': {\n",
    "            'bus': 'yes',\n",
    "            'highway': 'bus_stop',\n",
    "            'name': stops[0]['stop_name'].split('[')[0].strip(),\n",
    "            'public_transport': 'platform',\n",
    "            'ref': ';'.join(codes),\n",
    "            'shelter': 'yes' if stops[0]['stop_abribus'] == '1' else 'no',\n",
    "        },\n",
    "        \"geom\": point,\n",
    "        # GTFS fields\n",
    "        \"gtfs_props\": {\n",
    "            'stop_id': ','.join(ids),\n",
    "            'stop_code': ','.join(codes),\n",
    "            'stop_name': stops[0]['stop_name'].split('[')[0].strip(),\n",
    "            'stop_lon': lon,\n",
    "            'stop_lat': lat,\n",
    "            'location_type': stops[0]['location_type'],\n",
    "            'stop_display': stops[0]['stop_display'],\n",
    "            'stop_abribus': stops[0]['stop_abribus'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "#     unique_stops_map.append((ids, osm_id))\n",
    "\n",
    "    if action:\n",
    "        unique_stop['props']['action'] = action\n",
    "\n",
    "#     unique_stops_gtfs.append(unique_stop_gtfs)\n",
    "    unique_stops.append(unique_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge stops by proximity (tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:07:35.031685Z",
     "start_time": "2019-01-20T13:07:31.291408Z"
    }
   },
   "outputs": [],
   "source": [
    "buffer_distance = 2  # meter\n",
    "multi_point_utm = reproject_geometry(multi_point.Clone(), 4326, 32618)\n",
    "buffered_points = multi_point_utm.Buffer(5)\n",
    "buffered_points_dissolved = buffered_points.UnionCascaded()\n",
    "buffered_points_dissolved_geo = reproject_geometry(buffered_points_dissolved.Clone(), 32618, 4326)\n",
    "\n",
    "buffers_file = os.path.join(output_dir, 'buffers.geojson')\n",
    "write_geometry_to_geojson(buffered_points_dissolved_geo, buffers_file)\n",
    "\n",
    "stops_file = os.path.join(output_dir, 'gtfs_stops.geojson')\n",
    "dict_to_geojson(unique_stops, stops_file, 'geom', fields_key='gtfs_props', epsg_id=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:09:41.339711Z",
     "start_time": "2019-01-20T13:07:36.551276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fea546d8c442ac9d655d6e4a7ad17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique stops before merge: 2773\n",
      "Unique stops after merge: 2681\n"
     ]
    }
   ],
   "source": [
    "unique_stops_merged = []\n",
    "\n",
    "for i, buffer in tqdm_notebook(enumerate(buffered_points_dissolved_geo)):\n",
    "    stops_to_merge = []\n",
    "    for stop in unique_stops:\n",
    "        if stop['geom'].Within(buffer):\n",
    "            stops_to_merge.append(stop)\n",
    "            \n",
    "    if len(stops_to_merge) > 1:\n",
    "        new_stop = stops_to_merge[0].copy()\n",
    "        merged_codes = []\n",
    "        merged_ids = []\n",
    "        \n",
    "        for stop_to_merge in stops_to_merge:\n",
    "            merged_codes.extend(stop_to_merge['gtfs_props']['stop_code'].split(','))\n",
    "            merged_ids.extend(stop_to_merge['gtfs_props']['stop_id'].split(','))\n",
    "            \n",
    "            # Copy the first stop in the list and update the keys\n",
    "            new_stop['gtfs_props']['stop_code'] = ','.join(merged_codes)\n",
    "            new_stop['gtfs_props']['stop_id'] = ','.join(merged_ids)\n",
    "            new_stop['tags']['ref'] = ';'.join(merged_codes)\n",
    "            \n",
    "        unique_stops_merged.append(new_stop)\n",
    "        \n",
    "    if len(stops_to_merge) == 1:\n",
    "        unique_stops_merged.append(stops_to_merge[0])\n",
    "            \n",
    "print('Unique stops before merge: {}'.format(len(unique_stops)))\n",
    "print('Unique stops after merge: {}'.format(len(unique_stops_merged)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:09:53.101909Z",
     "start_time": "2019-01-20T13:09:52.865491Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_stops_merged_file = os.path.join(output_dir, 'unique_stops_merged.geojson')\n",
    "dict_to_geojson(unique_stops_merged, unique_stops_merged_file, 'geom', 'gtfs_props', 4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate GTFS routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:09:59.010790Z",
     "start_time": "2019-01-20T13:09:58.989738Z"
    }
   },
   "outputs": [],
   "source": [
    "routes_data = gtfs_data['routes']['data']\n",
    "stop_times_data = gtfs_data['stop_times']['data']\n",
    "trips_data = gtfs_data['trips']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:10:01.974143Z",
     "start_time": "2019-01-20T13:10:01.955265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find all trips for a route\n",
    "route_trips = defaultdict(set)\n",
    "\n",
    "for trip in trips_data:\n",
    "    route_trips[trip['route_id']].add(trip['trip_id'])\n",
    "    \n",
    "route_trips = dict(route_trips)\n",
    "    \n",
    "for route_id, trip_ids_set in route_trips.items():\n",
    "    route_trips[route_id] = list(trip_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:10:05.139881Z",
     "start_time": "2019-01-20T13:10:04.205663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d41bbf5258744318768ef6f99d95c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=667585), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stops per trip\n",
    "stops_counter = Counter()\n",
    "\n",
    "for stop_time in tqdm_notebook(stop_times_data):\n",
    "    stops_counter[stop_time['trip_id']] += 1\n",
    "\n",
    "stops_counter = dict(stops_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:18:59.608477Z",
     "start_time": "2019-01-20T13:18:56.277773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bb59962c364464876b02fa3daba625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the longest trip per route\n",
    "longest_route_trips = []\n",
    "\n",
    "for route_id, trip_ids in tqdm_notebook(route_trips.items()):\n",
    "    trips = []\n",
    "    \n",
    "    for trip_id, stop_count in stops_counter.items():\n",
    "        if trip_id in trip_ids:\n",
    "            trips.append((trip_id, stop_count))\n",
    "            \n",
    "    trips.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    longest_route_trips.append({\n",
    "        \"route_id\": route_id,\n",
    "        \"trip_id\": trips[0][0],\n",
    "        \"stops_count\": trips[0][1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:19:03.450888Z",
     "start_time": "2019-01-20T13:19:03.439766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'route_id': 'JUIN18901N',\n",
       " 'stops_count': 19,\n",
       " 'trip_id': 'JUIN18901N2L9817290007'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_route_trips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:19:31.484372Z",
     "start_time": "2019-01-20T13:19:06.096165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cbb2e732004528b6f7f12f98ba8f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the stop ids to the longest route\n",
    "for longest_trip in tqdm_notebook(longest_route_trips):\n",
    "    trip_id = longest_trip['trip_id']\n",
    "    stops = []\n",
    "    \n",
    "    for stop_time in stop_times_data:\n",
    "        if stop_time['trip_id'] == trip_id:\n",
    "#             stop_ids.append((stop_time['stop_id'], int(stop_time['stop_sequence'])))\n",
    "            stops.append({\n",
    "                \"gtfs_id\": stop_time['stop_id'],\n",
    "                \"sequence\": int(stop_time['stop_sequence'])\n",
    "            })\n",
    "            \n",
    "#     stop_ids.sort(key=itemgetter(1))\n",
    "    stop_ids.sort(key=itemgetter('sequence'))\n",
    "    longest_trip['stops'] = stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:19:42.308441Z",
     "start_time": "2019-01-20T13:19:42.286273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'route_id': 'JUIN18901N',\n",
       " 'stops': [{'gtfs_id': 'JUIN18CP47901', 'sequence': 1},\n",
       "  {'gtfs_id': 'JUIN1846515', 'sequence': 2},\n",
       "  {'gtfs_id': 'JUIN18CP41705', 'sequence': 3},\n",
       "  {'gtfs_id': 'JUIN1841711', 'sequence': 4},\n",
       "  {'gtfs_id': 'JUIN1841213', 'sequence': 5},\n",
       "  {'gtfs_id': 'JUIN1842176', 'sequence': 6},\n",
       "  {'gtfs_id': 'JUIN1842410', 'sequence': 7},\n",
       "  {'gtfs_id': 'JUIN1842419', 'sequence': 8},\n",
       "  {'gtfs_id': 'JUIN1842417', 'sequence': 9},\n",
       "  {'gtfs_id': 'JUIN18CP42415', 'sequence': 10},\n",
       "  {'gtfs_id': 'JUIN1845012', 'sequence': 11},\n",
       "  {'gtfs_id': 'JUIN1845013', 'sequence': 12},\n",
       "  {'gtfs_id': 'JUIN1846301', 'sequence': 13},\n",
       "  {'gtfs_id': 'JUIN1842458', 'sequence': 14},\n",
       "  {'gtfs_id': 'JUIN1842456', 'sequence': 15},\n",
       "  {'gtfs_id': 'JUIN1842453', 'sequence': 16},\n",
       "  {'gtfs_id': 'JUIN1842449', 'sequence': 17},\n",
       "  {'gtfs_id': 'JUIN1845059', 'sequence': 18},\n",
       "  {'gtfs_id': 'JUIN18CP49901', 'sequence': 19}],\n",
       " 'stops_count': 19,\n",
       " 'trip_id': 'JUIN18901N2L9817290007'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_route_trips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:25:00.653959Z",
     "start_time": "2019-01-20T13:23:57.967227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41392d9a8d5b45d78ebccc843d880337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the GTFS stops to the merged unique stops\n",
    "for longest_trip in tqdm_notebook(longest_route_trips):\n",
    "    for gtfs_stop in longest_trip['stops']:\n",
    "        for unique_stop in unique_stops_merged:\n",
    "            unique_stop_ids = unique_stop['gtfs_props']['stop_id'].split(',')\n",
    "            osm_id = unique_stop['props']['id']\n",
    "            name = unique_stop['tags']['name']\n",
    "            if gtfs_stop['gtfs_id'] in unique_stop_ids:\n",
    "                gtfs_stop.update({\n",
    "                    \"osm_id\": osm_id,\n",
    "                    \"name\": name\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:25:31.380651Z",
     "start_time": "2019-01-20T13:25:31.334405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gtfs_id': 'JUIN18CP47901',\n",
       "  'name': 'Métro Cartier Quai:6',\n",
       "  'osm_id': '-2314',\n",
       "  'sequence': 1},\n",
       " {'gtfs_id': 'JUIN1846515',\n",
       "  'name': 'Bretelle Autoroute 440 / Masson',\n",
       "  'osm_id': '-1610',\n",
       "  'sequence': 2},\n",
       " {'gtfs_id': 'JUIN18CP41705',\n",
       "  'name': \"De L'harmonie / Marcel-Villeneuve\",\n",
       "  'osm_id': '-1355',\n",
       "  'sequence': 3},\n",
       " {'gtfs_id': 'JUIN1841711',\n",
       "  'name': \"De L'harmonie / Face Au 1360\",\n",
       "  'osm_id': '-1583',\n",
       "  'sequence': 4},\n",
       " {'gtfs_id': 'JUIN1841213',\n",
       "  'name': 'Léa / Chimène',\n",
       "  'osm_id': '-359',\n",
       "  'sequence': 5},\n",
       " {'gtfs_id': 'JUIN1842176',\n",
       "  'name': 'Chimène / Mirelle',\n",
       "  'osm_id': '-111',\n",
       "  'sequence': 6},\n",
       " {'gtfs_id': 'JUIN1842410',\n",
       "  'name': 'Mirelle / Tristan',\n",
       "  'osm_id': '-647',\n",
       "  'sequence': 7},\n",
       " {'gtfs_id': 'JUIN1842419',\n",
       "  'name': 'Mirelle / Romain',\n",
       "  'osm_id': '-2175',\n",
       "  'sequence': 8},\n",
       " {'gtfs_id': 'JUIN1842417',\n",
       "  'name': 'Mirelle / Adrien',\n",
       "  'osm_id': '-2021',\n",
       "  'sequence': 9},\n",
       " {'gtfs_id': 'JUIN18CP42415',\n",
       "  'name': 'Mirelle / Chartrand',\n",
       "  'osm_id': '-1112',\n",
       "  'sequence': 10},\n",
       " {'gtfs_id': 'JUIN1845012',\n",
       "  'name': 'Chartrand / Monty',\n",
       "  'osm_id': '-507',\n",
       "  'sequence': 11},\n",
       " {'gtfs_id': 'JUIN1845013',\n",
       "  'name': 'Chartrand / Déry',\n",
       "  'osm_id': '-221',\n",
       "  'sequence': 12},\n",
       " {'gtfs_id': 'JUIN1846301',\n",
       "  'name': 'Chartrand / Montée Du Moulin',\n",
       "  'osm_id': '-1130',\n",
       "  'sequence': 13},\n",
       " {'gtfs_id': 'JUIN1842458',\n",
       "  'name': \"Montée Du Moulin / De L'église\",\n",
       "  'osm_id': '-2098',\n",
       "  'sequence': 14},\n",
       " {'gtfs_id': 'JUIN1842456',\n",
       "  'name': 'Montée Du Moulin / Romain',\n",
       "  'osm_id': '-2737',\n",
       "  'sequence': 15},\n",
       " {'gtfs_id': 'JUIN1842453',\n",
       "  'name': 'Montée Du Moulin / De Léry',\n",
       "  'osm_id': '-2509',\n",
       "  'sequence': 16},\n",
       " {'gtfs_id': 'JUIN1842449',\n",
       "  'name': 'Montée Du Moulin / Marius-Barbeau',\n",
       "  'osm_id': '-2060',\n",
       "  'sequence': 17},\n",
       " {'gtfs_id': 'JUIN1845059',\n",
       "  'name': 'Montée Du Moulin / La Périère',\n",
       "  'osm_id': '-1461',\n",
       "  'sequence': 18},\n",
       " {'gtfs_id': 'JUIN18CP49901',\n",
       "  'name': 'Galeries Du Moulin',\n",
       "  'osm_id': '-119',\n",
       "  'sequence': 19}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_route_trips[0]['stops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:30:43.426085Z",
     "start_time": "2019-01-20T13:30:43.416002Z"
    }
   },
   "outputs": [],
   "source": [
    "for route in longest_route_trips:\n",
    "    for stop in route['stops']:\n",
    "        if 'name' not in stop:\n",
    "            print('no name')\n",
    "            print(stop)\n",
    "\n",
    "            if stop['gtfs_id'] == 'AOUT18CP40002':\n",
    "\n",
    "                stop.update({\n",
    "                    'name': 'Terminus Henri-Bourassa'\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the route names and other info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T13:55:11.649717Z",
     "start_time": "2019-01-20T13:55:11.643671Z"
    }
   },
   "outputs": [],
   "source": [
    "route_relations = []\n",
    "route_master_relations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:45:04.592738Z",
     "start_time": "2019-01-20T18:45:04.541246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31081c5ddadc48e9b8961261d7160b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "osm_new_route_id = -10000\n",
    "\n",
    "for i, longest_route in enumerate(tqdm_notebook(longest_route_trips)):\n",
    "    route_id = longest_route['route_id']\n",
    "    route_stops = longest_route['stops']\n",
    "    \n",
    "    first_stop = route_stops[0]['name']\n",
    "    last_stop = route_stops[-1]['name']\n",
    "\n",
    "    loop_route = 'no'\n",
    "\n",
    "    if first_stop == last_stop:\n",
    "        loop_route = 'yes'\n",
    "\n",
    "    for route in routes_data:\n",
    "        if route_id == route['route_id']:\n",
    "            osm_new_route_id -= 1\n",
    "            longest_route.update({\n",
    "                \"props\": {\n",
    "                    \"id\": osm_new_route_id\n",
    "                },\n",
    "                \"tags\": {\n",
    "                    \"name\": route['route_long_name'],\n",
    "                    \"ref\": route[\"route_headsign\"],\n",
    "                    \"type\": \"route\",\n",
    "                    \"route\": \"bus\",\n",
    "                    \"network\": \"STL\",\n",
    "                    \"operator\": \"STL\",\n",
    "                    \"from\": first_stop,\n",
    "                    \"to\": last_stop,\n",
    "                    \"round_trip\": loop_route,\n",
    "                    \"public_transport:version\": 2\n",
    "                },\n",
    "                \"master_route_ref\": route['route_short_name']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:45:08.588975Z",
     "start_time": "2019-01-20T18:45:08.571673Z"
    }
   },
   "outputs": [],
   "source": [
    "master_routes = defaultdict(list)\n",
    "\n",
    "for route in routes_data:\n",
    "    master_routes[route['route_short_name']].append(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:57:08.172965Z",
     "start_time": "2019-01-20T18:57:08.154824Z"
    }
   },
   "outputs": [],
   "source": [
    "osm_new_master_route_id = -100000\n",
    "\n",
    "for ref, routes in master_routes.items():\n",
    "    osm_new_master_route_id -= 1\n",
    "    names = set()\n",
    "    route_ids = []\n",
    "    for route in routes:\n",
    "        names.add(route['route_long_name'].split('Direction')[1].strip())\n",
    "        route_ids.append(route['route_id'])\n",
    "        \n",
    "    names = list(names)\n",
    "        \n",
    "    if len(names) == 2:\n",
    "        name = '{} - {}'.format(names[0], names[1])\n",
    "    elif len(names) == 1:\n",
    "        name = names[0]\n",
    "    else:\n",
    "        print('error')\n",
    "    \n",
    "    route_master_relation = {\n",
    "        \"props\": {\n",
    "            \"id\": osm_new_master_route_id\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"name\": name,\n",
    "            \"ref\": ref,\n",
    "            \"network\": 'STL',\n",
    "            \"operator\": 'STL',\n",
    "            \"type\": \"route_master\",\n",
    "            \"public_transport:version\": 2\n",
    "        }\n",
    "    }\n",
    "    route_master_relations.append(route_master_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T16:27:35.199585Z",
     "start_time": "2019-01-13T16:27:34.931646Z"
    }
   },
   "outputs": [],
   "source": [
    "root = ET.Element(\"osm\")\n",
    "root.set('version', '0.6')\n",
    "\n",
    "min_lon, max_lon, min_lat, max_lat = multi_point.GetEnvelope()\n",
    "\n",
    "bounds = ET.SubElement(root, 'bounds')\n",
    "bounds.set('minlat', str(min_lat))\n",
    "bounds.set('minlon', str(min_lon))\n",
    "bounds.set('maxlat', str(max_lat))\n",
    "bounds.set('maxlon', str(max_lon))\n",
    "\n",
    "for stop in unique_stops_osm:\n",
    "    node = ET.SubElement(root, 'node')\n",
    "    node.set('version', '1')\n",
    "    \n",
    "    for key, value in stop.items():\n",
    "        if key == 'tags':\n",
    "            for k, v in stop['tags'].items():\n",
    "                tag = ET.SubElement(node, 'tag')\n",
    "                tag.set('k', str(k))\n",
    "                tag.set('v', str(v))\n",
    "        else:\n",
    "            node.set(key, value)\n",
    "\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"gtfs_laval.xml\", encoding='unicode')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
